{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom numpy import expand_dims\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras import Sequential\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(Conv2D(16,(3,3), activation='relu', input_shape=(28,28,1),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(32,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(5,5),activation='relu',padding='same')) \nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(5,5),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(5,5),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((3,3)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,(5,5),activation='relu',padding='same')) \nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((3,3)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom numpy import expand_dims\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras import Sequential\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\ntrain = pd.read_csv('/kaggle/input/mniststt/train.csv')\ntest = pd.read_csv('/kaggle/input/mniststt/test.csv')\nsub = pd.read_csv('/kaggle/input/mniststt/submission.csv')\n\n# check the distribution of 'digit'\ntrain['digit'].value_counts()\n\n# drop columns\ntrain2 = train.drop(['id','digit','letter'],1)\ntest2 = test.drop(['id','letter'],1)\n\n# convert pandas dataframe to numpy array\ntrain2 = train2.values\ntest2 = test2.values\n\nplt.imshow(train2[100].reshape(28,28))\n\ntrain2 = train2.reshape(-1,28,28,1)\ntest2 = test2.reshape(-1,28,28,1)\n\n# normalization\ntrain2 = train2/255.\ntest2 = test2/255.\n\n# image data generator * data augmentation\nidg = ImageDataGenerator(height_shift_range = (-1,1), width_shift_range = (-1,1))\nidg2 = ImageDataGenerator()\n\n# show augmented image data\nsample_data = train2[100].copy()\nsample = expand_dims(sample_data,0)\nsample_datagen = ImageDataGenerator(height_shift_range=(-3,3), width_shift_range=(3,-3))\nsample_generator = sample_datagen.flow(sample, batch_size=1)\n\nplt.figure(figsize=(16,10))\n\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    sample_batch = sample_generator.next()\n    sample_image = sample_batch[0]\n    plt.imshow(sample_image.reshape(28,28))\n\n# cross validation\nskf = StratifiedKFold(n_splits=40, random_state=42, shuffle=True)\n\n# patience 만큼의 epoch에도 벗어나지 못하면 callback 호출\n# verbose\n# factor: 학습률을 1/2로 줄인다 patience 만큼의 epoch에도 벗어나지 못하면 callback 호출\nreLR = ReduceLROnPlateau(monitor='accuracy',patience=100, verbose=1, factor=0.5)\nes = EarlyStopping(patience=160, verbose=1)\n\nval_loss_min = []\nresult=0\nnth = 0\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfor train_idx, valid_idx in tqdm(skf.split(train2,train['digit'])):\n    mc = ModelCheckpoint('best_cvision.h5', save_best_only=True, verbose=1)\n\n    x_train = train2[train_idx]\n    x_valid = train2[valid_idx]\n    y_train = train['digit'][train_idx]\n    y_valid = train['digit'][valid_idx]\n\n    train_generator = idg.flow(x_train, y_train, batch_size=8)\n    valid_generator = idg2.flow(x_valid, y_valid)\n    test_generator = idg2.flow(test2,shuffle=False)\n\n    model = Sequential()\n    model.add(Conv2D(16,(3,3), activation='relu', input_shape=(28,28,1),padding='same'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,(5,5),activation='relu',padding='same')) \n    model.add(BatchNormalization())\n    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,(5,5),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((3,3)))\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,(5,5),activation='relu',padding='same')) \n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((3,3)))\n    model.add(Dropout(0.3))\n\n    model.add(Flatten())\n\n    model.add(Dense(128,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    model.add(Dense(64,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Dense(10, activation='softmax'))\n\n    # model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.002,epsilon=None), metrics=['acc'])\n    model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.002,epsilon=None), metrics=['acc'])\n\n    # learning_history = model.fit_generator(train_generator, \n    #                                        epochs=2000, \n    #                                        validation_data=valid_generator,\n    #                                        callbacks=[es,mc,reLR])\n\n    learning_history = model.fit(train_generator, \n                                         epochs=2000, \n                                         validation_data=valid_generator,\n                                         callbacks=[es,mc,reLR])\n\n    # predict\n    model.load_weights('best_cvision.h5')\n    result += model.predict_generator(test_generator,verbose=True)\n\n    # save val_loss\n    hist = pd.DataFrame(learning_history.history)\n    val_loss_min.append(hist['val_loss'].min())\n\n    nth += 1\n    print(nth, '번째 학습을 완료했습니다.')\n\ndisplay(val_loss_min, np.mean(val_loss_min))model.summary()\n\nsub.to_csv('result.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['digit'] = result.argmax(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('Dacon_cvision_0914_40_epsNone.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}